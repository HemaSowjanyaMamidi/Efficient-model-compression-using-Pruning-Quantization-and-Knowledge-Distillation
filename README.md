# Efficient-model-compression-using-Pruning-Quantization-and-Knowledge-Distillation

<p align="justify">
  The state of the art neural network models have billion number of parameters and require huge amounts of memory and computational power for their usage. This poses a huge problem when these models need to be deployed on resource constraint devices like mobiles, wearable devices etc., In order to deploy them on the edge devices the models should be compressed and optimized. Some of the compression methods like pruning, quantization and knowledge distillation were used and compared against the optimization techniques that are available in the Tensorflow Model Optimization Library.
